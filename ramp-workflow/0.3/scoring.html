

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>RAMP scoring &mdash; RAMP 0.3.1 documentation</title>
  

  
  
  
  

  
  <script type="text/javascript" src="_static/js/modernizr.min.js"></script>
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="./" src="_static/documentation_options.js"></script>
        <script src="_static/jquery.js"></script>
        <script src="_static/underscore.js"></script>
        <script src="_static/doctools.js"></script>
        <script src="_static/language_data.js"></script>
        <script src="_static/js/copybutton.js"></script>
    
    <script type="text/javascript" src="_static/js/theme.js"></script>

    

  
  <link rel="stylesheet" href="_static/css/ramp.css" type="text/css" />
  <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="_static/basic.css" type="text/css" />
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="RAMP workflow commands" href="command_line.html" />
    <link rel="prev" title="The problem.py file" href="problem.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="index.html" class="icon icon-home"> RAMP
          

          
          </a>

          
            
            
              <div class="version">
                0.3.1
              </div>
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">What can RAMP offer you?</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="who_are_you.html">Who are you</a></li>
</ul>
<p class="caption"><span class="caption-text">Using RAMP workflow</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="workflow.html">Build your own RAMP challenge</a></li>
<li class="toctree-l1"><a class="reference internal" href="data.html">Preparing your data</a></li>
<li class="toctree-l1"><a class="reference internal" href="problem.html">The problem.py file</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">RAMP scoring</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#local-submissions">Local submissions</a></li>
<li class="toctree-l2"><a class="reference internal" href="#ramp-event-submissions">RAMP event submissions</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="command_line.html">RAMP workflow commands</a></li>
<li class="toctree-l1"><a class="reference internal" href="contribute.html">Contributing</a></li>
</ul>
<p class="caption"><span class="caption-text">Using RAMP kits</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="using_kits.html">Using RAMP starting-kits</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">RAMP</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="index.html">Docs</a> &raquo;</li>
        
      <li>RAMP scoring</li>
    
    

  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="ramp-scoring">
<span id="scoring"></span><h1>RAMP scoring<a class="headerlink" href="#ramp-scoring" title="Permalink to this headline">¶</a></h1>
<div class="section" id="local-submissions">
<h2>Local submissions<a class="headerlink" href="#local-submissions" title="Permalink to this headline">¶</a></h2>
<p>When testing a submission locally (i.e. with <code class="docutils literal notranslate"><span class="pre">ramp_test_submission</span></code>) a number
of scores will be calculated and printed to standard output. The scores will
look like this:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>Testing Titanic survival classification
Reading train and test files from ./data ...
Reading cv ...
Training ./submissions/random_forest_20_5 ...
CV fold 0
    train auc = 0.84
    valid auc = 0.89
    test auc = 0.83
CV fold 1
    train auc = 0.85
    valid auc = 0.86
    test auc = 0.83
CV fold 2
    train auc = 0.85
    valid auc = 0.83
    test auc = 0.82
CV fold 3
    train auc = 0.84
    valid auc = 0.91
    test auc = 0.83
CV fold 4
    train auc = 0.85
    valid auc = 0.87
    test auc = 0.83
CV fold 5
    train auc = 0.84
    valid auc = 0.89
    test auc = 0.84
CV fold 6
    train auc = 0.84
    valid auc = 0.88
    test auc = 0.84
CV fold 7
    train auc = 0.85
    valid auc = 0.86
    test auc = 0.84
----------------------------
Mean CV scores
----------------------------
    train auc = 0.85 ± 0.005
    valid auc = 0.87 ± 0.023
    test auc = 0.83 ± 0.006
----------------------------
Bagged scores
----------------------------
    score   auc
    valid  0.875
    test   0.834
</pre></div>
</div>
<p>Locally, there should be a training dataset and a testing dataset, usually
within a folder named <code class="docutils literal notranslate"><span class="pre">data/</span></code>. We will call these datasets the ‘public’
training data and the ‘public’ test data. This is because, for a RAMP challenge,
there will also be private training and test data (see <a class="reference internal" href="data.html#data"><span class="std std-ref">Preparing your data</span></a> for more).</p>
<p>Eight-fold cross-validation (CV) is performed, whereby the public training data
is split into ‘training’ and ‘validation’ subsets 8 times. The subsets are
different each time. For each CV fold, the model is trained with
the training data then used to predict targets for the training and validation
subsets and the public testing data. The scores are computed for the training,
validation and testing datasets, for each fold. The mean of these 8 scores are
calculated and printed under <code class="docutils literal notranslate"><span class="pre">Mean</span> <span class="pre">CV</span> <span class="pre">scores</span></code>. In the example above, there is
only one score metric; ‘auc’. If more than one score metric was defined in
<code class="docutils literal notranslate"><span class="pre">problem.py</span></code> (see <a class="reference internal" href="problem.html#score-types"><span class="std std-ref">score types</span></a>), scores for all the score
metrics will be printed.</p>
<p><code class="docutils literal notranslate"><span class="pre">Bagged</span> <span class="pre">scores</span></code> are calculated by combining the predictions of the 8 folds
and using the combined prediction to calculate the score. For regression
problems the combined prediction is the mean of the predictions and
for classification problems, it is the mean probability of each class. For
detection problems, the combined prediction calculation is more complex. See
the <a class="reference external" href="https://github.com/paris-saclay-cds/ramp-workflow/blob/12512a3192bcc515c2da956a6a6704849cdadeee/rampwf/prediction_types/detection.py#L37">source code</a>
for more details.</p>
<p>For example, the Titanic challenge aims to predict whether or not each
passenger survived. For each CV fold, different survival predictions are made
for the test data. This is because for each CV fold, the model is different as
it was trained using different data. The probality of each classification
(survived or did not survive), computed from the 8 CV models, is averaged for
every sample in the test dataset. The classification label is
then computed using the new averaged probabilities. This new ‘combined
prediction’ is used to calculate the ‘bagged’ score. The validation bagged
score is calculated similarly, though there is a slight variation because the
validation datasets differ between each CV fold. Validation
samples may or may not overlap between CV folds. In cases where a validation
sample was present in only one CV fold, there is only one prediction for that
sample. The combined prediction for this sample will simply be that single
prediction.</p>
<p>Note that technically this is not what bagging means, but the name is used for
historical reasons.</p>
</div>
<div class="section" id="ramp-event-submissions">
<h2>RAMP event submissions<a class="headerlink" href="#ramp-event-submissions" title="Permalink to this headline">¶</a></h2>
<p>The above scores are also calculated when you make a submission to a RAMP
event on <a class="reference external" href="https://ramp.studio/">RAMP studio</a>. However, only the mean cv validation score (i.e.,
<code class="docutils literal notranslate"><span class="pre">valid</span>&#160; <span class="pre">0.825</span> <span class="pre">±</span> <span class="pre">0.0096</span></code> above) is shown on the public leaderboard. The
mean cv test score is not shown as we wish to assess if the participants
submissions generalise to the private test data. Providing them with the
test score provides participants with a score to try and improve and may result
in models that perform well on the test data because it is overfit for the test
data.</p>
<p>Typically, the test score is used to officially rank the participants and
are made public at the end of a RAMP event.</p>
</div>
</div>


           </div>
           
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="command_line.html" class="btn btn-neutral float-right" title="RAMP workflow commands" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="problem.html" class="btn btn-neutral float-left" title="The problem.py file" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2015 - 2019, Paris-Saclay Center for Data Science

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>